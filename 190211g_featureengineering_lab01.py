# -*- coding: utf-8 -*-
"""190211G_FeatureEngineering_lab01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VM8EYx2mx_G2K4A6llIgRsUPCXYr68iL
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

labels=['label_1','label_2','label_3','label_4']
features=[f'feature_{i}' for i in range(1,257)]

train = pd.read_csv("/content/drive/MyDrive/Dataset1/train.csv")
train.head()

valid = pd.read_csv("/content/drive/MyDrive/Dataset1/valid.csv")
valid.head()

from sklearn.preprocessing import StandardScaler

X_train={}
X_valid={}
y_train={}
y_valid={}

for y in labels:
  if y=='label_2':
    train_df=train.dropna()
    print(train_df)
    valid_df=valid.dropna()
  else:
    train_df=train
    valid_df=valid
  scaler=StandardScaler()

  X_train[y]=pd.DataFrame(scaler.fit_transform(train_df.drop(labels,axis=1)),columns=features)
  y_train[y]=train_df[y]
  X_valid[y]=pd.DataFrame(scaler.transform(valid_df.drop(labels,axis=1)),columns=features)
  y_valid[y]=valid_df[y]

y_train['label_2']

"""#**Dropping constant features**"""

from sklearn.feature_selection import VarianceThreshold
var_thres=VarianceThreshold(threshold=0)
var_thres.fit(X_train['label_1'])

X_train['label_1'].columns[var_thres.get_support()]

constant_columns = [column for column in X_train['label_1'].columns
                    if column not in X_train['label_1'].columns[var_thres.get_support()]]

print(len(constant_columns))

"""
# **Checking Correlation between given Features in the dataset**
"""

cor = X_train['label_1'].corr()
plt.figure(figsize=(100,60))
sns.heatmap(cor, annot=True)

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train['label_1'], 0.85)
len(set(corr_features))
corr_features

"""#**Analysing the accuracy before using any feature selection tecniques**"""

from sklearn.svm import SVC
classifier=SVC(kernel='linear')
classifier.fit(X_train['label_1'],y_train['label_1'])

y_pred=classifier.predict(X_valid['label_1'])

from sklearn import metrics
print(metrics.accuracy_score(y_valid['label_1'],y_pred))
print(metrics.precision_score(y_valid['label_1'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_1'],y_pred, average='weighted'))

"""#**Univariate Feature Selection**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
selector = SelectKBest(f_classif, k=120)
X_new_L1 = selector.fit_transform(X_train['label_1'], y_train['label_1'])

X_new_L1.shape

classifier.fit(X_new_L1,y_train['label_1'])

y_pred=classifier.predict(selector.transform(X_valid['label_1']))

from sklearn import metrics
print(metrics.accuracy_score(y_valid['label_1'],y_pred))
print(metrics.precision_score(y_valid['label_1'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_1'],y_pred, average='weighted'))

"""#**Fisher's Score**"""

from skfeature.function.similarity_based import fisher_score
import pandas as pd
import matplotlib.pyplot as plt

# Assuming X_train and y_train are dictionaries where 'label_1' corresponds to the data for label 1.
# Make sure that X_train['label_1'] and y_train['label_1'] have the same number of samples.

# Compute Fisher scores
ranks = fisher_score.fisher_score(X_train['label_1'], y_train['label_1'])

# Create a pandas Series for visualization
fisher_imp1 = pd.Series(ranks, index=X_train['label_1'].columns)

# Plot the Fisher scores
fisher_imp1.plot(kind="barh")
plt.show()

!pip install skfeature-chappers

"""#**Principal Component Analysis**"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.95,svd_solver='full')
pca.fit(X_train['label_1'])
X_train_trf=pd.DataFrame(pca.transform(X_train['label_1']))
X_valid_trf=pd.DataFrame(pca.transform(X_valid['label_1']))
print('Shape after PCA :',X_train_trf.shape)

classifier.fit(X_train_trf,y_train['label_1'])

y_pred=classifier.predict(pca.transform(X_valid['label_1']))

print(metrics.accuracy_score(y_valid['label_1'],y_pred))
print(metrics.precision_score(y_valid['label_1'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_1'],y_pred, average='weighted'))

"""# **Label 2**

---







---

#**Analysing the accuracy before using any feature selection tecniques - Label 2**

#**As regression problem**
"""

import numpy as np
import pandas as pd
import xgboost as xg

xgb_r = xg.XGBRegressor(objective ='reg:linear',
                  n_estimators = 10, seed = 123)
xgb_r.fit(X_train['label_2'],y_train['label_2'])

# Predict the model
y_pred = xgb_r.predict(X_valid['label_2'])

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
mae = mean_absolute_error(y_valid['label_2'], y_pred)
mse = mean_squared_error(y_valid['label_2'], y_pred)
r2 = r2_score(y_valid['label_2'], y_pred)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("R-squared (R2) Score:", r2)

"""#**Univariate Feature Selection**"""

import numpy as np
from sklearn.feature_selection import SelectKBest, f_regression
selector = SelectKBest(score_func=f_regression, k=100)

# Fit the selector to your data and target
X_train_new=selector.fit_transform(X_train['label_2'],y_train['label_2'])

# Get the indices of the selected features
selected_indices = np.where(selector.get_support())[0]

# Get the names of the selected features (if you have feature names)
selected_feature_names = X_train['label_2'].columns[selected_indices]
# Print the selected feature names and their scores
print("Selected Feature Names:", selected_feature_names)
print("Feature Scores:", selector.scores_)

xgb_r.fit(X_train_new,y_train['label_2'])

y_pred=xgb_r.predict(selector.transform(['label_2']))

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
mae = mean_absolute_error(y_valid['label_2'], y_pred)
mse = mean_squared_error(y_valid['label_2'], y_pred)
r2 = r2_score(y_valid['label_2'], y_pred)
print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("R-squared (R2) Score:", r2)

"""#**As Classification**"""

from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(random_state=0)
clf.fit(X_train['label_2'],y_train['label_2'])

y_pred=clf.predict(X_valid['label_2'])

from sklearn import metrics
print(metrics.accuracy_score(y_valid['label_2'],y_pred))
print(metrics.precision_score(y_valid['label_2'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_2'],y_pred, average='weighted'))

"""#**Univariate Feature Selection**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
selector_2 = SelectKBest(f_classif, k=200)
X_new_L2 = selector_2.fit_transform(X_train['label_2'], y_train['label_2'])

clf.fit(X_new_L2,y_train['label_2'])

y_pred=clf.predict(selector_2.transform(X_valid['label_2']))

print(metrics.accuracy_score(y_valid['label_2'],y_pred))
print(metrics.precision_score(y_valid['label_2'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_2'],y_pred, average='weighted'))

""" # **Principal Component Analysis**"""

from sklearn.decomposition import PCA
pca_2 = PCA(n_components=0.95,svd_solver='full')
pca_2.fit(X_train['label_2'])
X_train_trf=pd.DataFrame(pca_2.transform(X_train['label_2']))
X_valid_trf=pd.DataFrame(pca_2.transform(X_valid['label_2']))
print('Shape after PCA :',X_train_trf.shape)

clf.fit(X_train_trf,y_train['label_2'])

y_pred=clf.predict(pca_2.transform(X_valid['label_2']))

print(metrics.accuracy_score(y_valid['label_2'],y_pred))
print(metrics.precision_score(y_valid['label_2'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_2'],y_pred, average='weighted'))

"""#**Label 3**

---



---

#**Analysing the accuracy before using any feature selection tecniques - Label 3**
"""

from sklearn.ensemble import RandomForestClassifier
clf_2=RandomForestClassifier(random_state=0)
clf_2.fit(X_train['label_3'],y_train['label_3'])

y_pred=clf_2.predict(X_valid['label_3'])

print(metrics.accuracy_score(y_valid['label_3'],y_pred))
print(metrics.precision_score(y_valid['label_3'],y_pred))
print(metrics.recall_score(y_valid['label_3'],y_pred))

unique_values = y_train['label_3'].value_counts()
print(unique_values)

from imblearn.under_sampling import RandomUnderSampler

# Assuming you have X_train and y_train
rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train['label_3'], y_train['label_3'])

unique_values = y_resampled.value_counts()
print(unique_values)

from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier(random_state=0)
clf.fit(X_resampled, y_resampled)

y_pred=clf.predict(X_valid['label_3'])

print(metrics.accuracy_score(y_valid['label_3'],y_pred))
print(metrics.precision_score(y_valid['label_3'],y_pred))
print(metrics.recall_score(y_valid['label_3'],y_pred))

"""#**Univariate Feature Selection**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
selector_3 = SelectKBest(f_classif, k=100)
X_new_L3 = selector_3.fit_transform(X_train['label_3'], y_train['label_3'])

clf_2.fit(X_new_L3, y_train['label_3'])

y_pred=clf_2.predict(selector_3.transform(X_valid['label_3']))

print(metrics.accuracy_score(y_valid['label_3'],y_pred))
print(metrics.precision_score(y_valid['label_3'],y_pred))
print(metrics.recall_score(y_valid['label_3'],y_pred))

"""#**Principal Component Analysis**"""

from sklearn.decomposition import PCA
pca_3 = PCA(n_components=0.96,svd_solver='full')
pca_3.fit(X_train['label_3'])
X_train_trf=pd.DataFrame(pca_3.transform(X_train['label_3']))
X_valid_trf=pd.DataFrame(pca_3.transform(X_valid['label_3']))
print('Shape after PCA :',X_train_trf.shape)

clf_2.fit(X_train_trf,y_train['label_3'])

y_pred=clf_2.predict(pca_3.transform(X_valid['label_3']))

print(metrics.accuracy_score(y_valid['label_3'],y_pred))
print(metrics.precision_score(y_valid['label_3'],y_pred))
print(metrics.recall_score(y_valid['label_3'],y_pred))

"""#**Label 4**

---



---

#**Analysing the accuracy before using any feature selection tecniques - Label 4**
"""

unique_values = y_train['label_4'].value_counts()
print(unique_values)

from sklearn.svm import SVC
classifier=SVC(kernel='linear',class_weight='balanced')
classifier.fit(X_train['label_4'],y_train['label_4'])

y_pred=classifier.predict(X_valid['label_4'])

print(metrics.accuracy_score(y_valid['label_4'],y_pred))
print(metrics.precision_score(y_valid['label_4'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_4'],y_pred, average='weighted'))

"""#**Univariate Feature Selection**"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
selector_4 = SelectKBest(f_classif, k=150)
X_new_L4 = selector_4.fit_transform(X_train['label_4'], y_train['label_4'])

X_new_L4.shape

classifier.fit(X_new_L4,y_train['label_4'])

y_pred=classifier.predict(selector_4.transform(X_valid['label_4']))

print(metrics.accuracy_score(y_valid['label_4'],y_pred))
print(metrics.precision_score(y_valid['label_4'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_4'],y_pred, average='weighted'))

"""#**Principal Component Analysis**"""

from sklearn.decomposition import PCA
pca_4 = PCA(n_components=0.96,svd_solver='full')
pca_4.fit(X_train['label_4'])
X_train_trf=pd.DataFrame(pca_3.transform(X_train['label_4']))
X_valid_trf=pd.DataFrame(pca_3.transform(X_valid['label_4']))
print('Shape after PCA :',X_train_trf.shape)

classifier.fit(X_train_trf,y_train['label_4'])

y_pred=classifier.predict(pca_4.transform(X_valid['label_4']))

print(metrics.accuracy_score(y_valid['label_4'],y_pred))
print(metrics.precision_score(y_valid['label_4'],y_pred, average='weighted'))
print(metrics.recall_score(y_valid['label_4'],y_pred, average='weighted'))